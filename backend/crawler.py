#!/usr/bin/env python3
"""
å…¨æ¯æ‹‰æ™®æ‹‰æ–¯äº’è”ç½‘çˆ¬è™«ç³»ç»Ÿ - æœ¬åœ°çˆ¬è™«è„šæœ¬
ä½¿ç”¨æ–¹æ³•:
1. ä»ç½‘é¡µç•Œé¢ä¸‹è½½é…ç½®æ–‡ä»¶ crawler_config.json
2. å°†é…ç½®æ–‡ä»¶æ”¾åœ¨æœ¬è„šæœ¬åŒä¸€ç›®å½•
3. è¿è¡Œè„šæœ¬: python crawler.py
4. è„šæœ¬ä¼šç”Ÿæˆç»“æœæ–‡ä»¶ crawler_results.json
5. å°†ç»“æœæ–‡ä»¶ä¸Šä¼ åˆ°ç½‘é¡µç•Œé¢æŸ¥çœ‹åˆ†æç»“æœ
"""

import os
import sys
import json
import time
import logging
import argparse
import hashlib
import re
import random
import numpy as np
from datetime import datetime
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor

try:
    import requests
    from bs4 import BeautifulSoup
    import nltk
    nltk.download('punkt_tab')
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
except ImportError:
    print("ç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:")
    print("pip install requests beautifulsoup4 nltk scikit-learn")
    sys.exit(1)

# åˆ›å»ºè‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†NumPyç±»å‹
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

# å¼ºåˆ¶æ§åˆ¶å°è¾“å‡ºä½¿ç”¨UTF-8
if sys.platform == 'win32':
    # ä¸ºWindowsç¯å¢ƒè®¾ç½®UTF-8ç¼–ç 
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("crawler.log", encoding='utf-8'),  # æŒ‡å®šæ–‡ä»¶ä½¿ç”¨UTF-8ç¼–ç 
        logging.StreamHandler(sys.stdout)  # ä½¿ç”¨å·²é…ç½®ä¸ºUTF-8çš„æ ‡å‡†è¾“å‡º
    ]
)
logger = logging.getLogger("Crawler")

# ç¡®ä¿NLTKèµ„æºå¯ç”¨
def ensure_nltk_resources():
    """æ£€æŸ¥å¹¶ç¡®ä¿æ‰€éœ€çš„NLTKèµ„æºå¯ç”¨ï¼Œåªåœ¨å¿…è¦æ—¶ä¸‹è½½"""
    resources = [
        ('tokenizers/punkt_tab', 'punkt_tab'),  # ç‰¹æ®Šèµ„æº
        ('tokenizers/punkt', 'punkt'),
        ('corpora/stopwords', 'stopwords'),
        ('corpora/wordnet', 'wordnet')
    ]
    
    missing_resources = []
    
    # æ£€æŸ¥å“ªäº›èµ„æºç¼ºå¤±
    for path, package in resources:
        try:
            nltk.data.find(path)
            logger.info(f"NLTKèµ„æºå·²å­˜åœ¨: {package}")
        except LookupError:
            missing_resources.append(package)
    
    # åªä¸‹è½½ç¼ºå¤±çš„èµ„æº
    if missing_resources:
        logger.info(f"ä¸‹è½½ç¼ºå¤±çš„NLTKèµ„æº: {', '.join(missing_resources)}")
        for package in missing_resources:
            try:
                print(f"ä¸‹è½½NLTKèµ„æº: {package}...")
                nltk.download(package, quiet=False)
                print(f"å·²æˆåŠŸä¸‹è½½: {package}")
            except Exception as e:
                logger.error(f"ä¸‹è½½NLTKèµ„æº {package} å¤±è´¥: {str(e)}")
                print(f"é”™è¯¯: æ— æ³•ä¸‹è½½ {package}, è¯¦æƒ…: {str(e)}")
    else:
        logger.info("æ‰€æœ‰å¿…éœ€çš„NLTKèµ„æºå·²å®‰è£…")

# æ›¿æ¢åŸä»£ç ä¸­çš„å¯¼å…¥å’ŒNLTKèµ„æºæ£€æŸ¥éƒ¨åˆ†
try:
    import requests
    from bs4 import BeautifulSoup
    import nltk
    
    # ç»Ÿä¸€ä½¿ç”¨ensure_nltk_resourceså‡½æ•°è¿›è¡Œèµ„æºæ£€æŸ¥å’Œä¸‹è½½
    # ä¸å†å•ç‹¬è°ƒç”¨nltk.download('punkt_tab')
    ensure_nltk_resources()
    
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
except ImportError:
    print("ç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:")
    print("pip install requests beautifulsoup4 nltk scikit-learn")
    sys.exit(1)


class WebCrawler:
    """ç½‘é¡µçˆ¬è™«ç±»ï¼Œè´Ÿè´£ä¸‹è½½å’Œè§£æç½‘é¡µ"""
    
    def __init__(self, max_workers=3, max_retries=3, timeout=30):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å‚æ•°:
            max_workers: æœ€å¤§å¹¶å‘æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        """
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.timeout = timeout
        self.session = requests.Session()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.visited_urls = set()
        
        # è®¾ç½®éšæœºç”¨æˆ·ä»£ç†
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
        ]
        
    def _get_random_user_agent(self):
        """è·å–éšæœºç”¨æˆ·ä»£ç†"""
        return random.choice(self.user_agents)
        
    def _get_random_delay(self, min_delay=1, max_delay=3):
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´"""
        return random.uniform(min_delay, max_delay)
        
    def download_page(self, url, retry_count=0):
        """
        ä¸‹è½½ç½‘é¡µå†…å®¹
        
        å‚æ•°:
            url: ç½‘é¡µURL
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        è¿”å›:
            (html_content, status_code) å…ƒç»„ï¼Œå¤±è´¥è¿”å› (None, status_code)
        """
        if retry_count >= self.max_retries:
            logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.max_retries}ï¼Œæ”¾å¼ƒURL: {url}")
            return None, 0
            
        # æ·»åŠ éšæœºå»¶è¿Ÿé˜²æ­¢è¢«å°
        time.sleep(self._get_random_delay())
        
        try:
            headers = {
                'User-Agent': self._get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # å¦‚æœURLä¸ä»¥httpå¼€å¤´ï¼Œæ·»åŠ åè®®
            if not url.startswith(('http://', 'https://')):
                url = 'http://' + url
                
            response = self.session.get(url, headers=headers, timeout=self.timeout)
            status_code = response.status_code
            
            if status_code == 200:
                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('Content-Type', '')
                
                # å¦‚æœæ˜¯ç½‘é¡µå†…å®¹
                if 'text/html' in content_type or 'application/xhtml+xml' in content_type:
                    # è‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼ˆå¦‚ gb2312, gbkï¼‰
                    response.encoding = response.apparent_encoding
                    return response.text, status_code
                
                # å…è®¸å¤„ç†HTMLå’ŒPDFç­‰å†…å®¹
                if 'text/html' in content_type or 'application/xhtml+xml' in content_type:
                    return response.text, status_code
                elif 'application/pdf' in content_type:
                    # æ ‡è®°ä¸ºPDFå¹¶è¿”å›å†…å®¹
                    logger.info(f"æ£€æµ‹åˆ°PDFæ–‡ä»¶: {url}")
                    return f"PDF_CONTENT_{url}", status_code
                else:
                    logger.warning(f"ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {url}, å†…å®¹ç±»å‹: {content_type}")
                    return f"UNSUPPORTED_CONTENT_{content_type}", status_code
            elif status_code in [403, 429]:
                # å¯èƒ½è¢«åçˆ¬ï¼Œå¢åŠ å»¶è¿Ÿåé‡è¯•
                retry_delay = self._get_random_delay(3, 10)
                logger.warning(f"å¯èƒ½è¢«åçˆ¬ï¼ŒçŠ¶æ€ç : {status_code}ï¼Œç­‰å¾… {retry_delay:.2f} ç§’åé‡è¯•: {url}")
                time.sleep(retry_delay)
                return self.download_page(url, retry_count + 1)
            else:
                logger.error(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {status_code}, URL: {url}")
                return None, status_code
                
        except requests.exceptions.Timeout:
            logger.warning(f"è¯·æ±‚è¶…æ—¶: {url}ï¼Œæ­£åœ¨é‡è¯• ({retry_count + 1}/{self.max_retries})")
            return self.download_page(url, retry_count + 1)
            
        except requests.exceptions.ConnectionError:
            logger.warning(f"è¿æ¥é”™è¯¯: {url}ï¼Œæ­£åœ¨é‡è¯• ({retry_count + 1}/{self.max_retries})")
            time.sleep(self._get_random_delay(2, 5))
            return self.download_page(url, retry_count + 1)
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å‡ºé”™: {url}, é”™è¯¯: {str(e)}")
            return None, 0
    
    def parse_html(self, html_content, url):
        """
        è§£æHTMLå†…å®¹ï¼ŒåŒ…æ‹¬æå–å†…åµŒåª’ä½“
        
        å‚æ•°:
            html_content: HTMLå†…å®¹å­—ç¬¦ä¸²
            url: åŸå§‹URLï¼ˆç”¨äºè§£æç›¸å¯¹é“¾æ¥ï¼‰
            
        è¿”å›:
            (title, content, links) å…ƒç»„ï¼Œmediaä¿¡æ¯å­˜å‚¨åœ¨contentä¸­
        """
        if not html_content:
            return None, None, []
            
        # å¤„ç†PDFæˆ–å…¶ä»–ä¸æ”¯æŒçš„å†…å®¹
        if isinstance(html_content, str) and html_content.startswith("PDF_CONTENT_"):
            return f"PDFæ–‡ä»¶: {url}", f"è¿™æ˜¯ä¸€ä¸ªPDFæ–‡ä»¶ï¼Œæ— æ³•ç›´æ¥æ˜¾ç¤ºå†…å®¹ã€‚ä¸‹è½½é“¾æ¥: {url}", []
        
        if isinstance(html_content, str) and html_content.startswith("UNSUPPORTED_CONTENT_"):
            content_type = html_content.replace("UNSUPPORTED_CONTENT_", "")
            return f"ä¸æ”¯æŒçš„å†…å®¹: {url}", f"è¿™æ˜¯ä¸€ä¸ª{content_type}ç±»å‹çš„æ–‡ä»¶ï¼Œæ— æ³•ç›´æ¥æ˜¾ç¤ºå†…å®¹ã€‚ä¸‹è½½é“¾æ¥: {url}", []
            
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = soup.title.text.strip() if soup.title else "æ— æ ‡é¢˜"
            
            # æå–æ­£æ–‡å†…å®¹ï¼ˆç®€å•å®ç°ï¼Œå¯ä¼˜åŒ–ï¼‰
            content_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'article'])
            content = "\n".join([tag.text.strip() for tag in content_tags])
            
            # æå–é“¾æ¥
            links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                # å°†ç›¸å¯¹URLè½¬ä¸ºç»å¯¹URL
                absolute_url = urljoin(url, href)
                # æ’é™¤é”šç‚¹é“¾æ¥å’ŒJavaScripté“¾æ¥
                if not href.startswith('#') and not href.startswith('javascript:'):
                    links.append(absolute_url)
            
            # æå–å†…åµŒåª’ä½“å†…å®¹
            media = extract_embedded_media(html_content, url)
            
            # å°†åª’ä½“ä¿¡æ¯æ·»åŠ åˆ°å†…å®¹ä¸­
            if media:
                media_content = "\n\nåµŒå…¥çš„åª’ä½“å†…å®¹:\n"
                
                if "youtube" in media:
                    media_content += "\nYouTubeè§†é¢‘:\n"
                    for youtube_url in media["youtube"]:
                        media_content += f"- {youtube_url}\n"
                
                if "twitter" in media:
                    media_content += "\nTwitterå†…å®¹:\n"
                    for twitter_url in media["twitter"]:
                        media_content += f"- {twitter_url}\n"
                
                if "vimeo" in media:
                    media_content += "\nVimeoè§†é¢‘:\n"
                    for vimeo_url in media["vimeo"]:
                        media_content += f"- {vimeo_url}\n"
                
                if "instagram" in media:
                    media_content += "\nInstagramå†…å®¹:\n"
                    for instagram_url in media["instagram"]:
                        media_content += f"- {instagram_url}\n"
                
                if "other_embedded" in media:
                    media_content += "\nå…¶ä»–åµŒå…¥å†…å®¹:\n"
                    for other_url in media["other_embedded"]:
                        media_content += f"- {other_url}\n"
                
                # å°†åª’ä½“å†…å®¹é™„åŠ åˆ°åŸå†…å®¹å
                content += media_content
                    
            return title, content, links
            
        except Exception as e:
            logger.error(f"è§£æHTMLå‡ºé”™: {url}, é”™è¯¯: {str(e)}")
            return None, None, []
    
    def crawl(self, url, depth=1):
        """
        çˆ¬å–ç½‘é¡µï¼Œæ”¯æŒæ·±åº¦çˆ¬å–
        
        å‚æ•°:
            url: èµ·å§‹URL
            depth: çˆ¬å–æ·±åº¦
            
        è¿”å›:
            çˆ¬å–ç»“æœå­—å…¸
        """
        if depth <= 0 or url in self.visited_urls:
            return {}
            
        self.visited_urls.add(url)
        result = {url: {'depth': 0, 'title': None, 'content': None, 'links': [], 'status': None}}
            
        # ä¸‹è½½é¡µé¢
        html_content, status_code = self.download_page(url)
        result[url]['status'] = status_code
        
        # è§£æå†…å®¹
        if html_content:
            title, content, links = self.parse_html(html_content, url)
            result[url]['title'] = title
            result[url]['content'] = content
            result[url]['links'] = links
            result[url]['html'] = html_content
            
            # å¦‚æœéœ€è¦ç»§ç»­æ·±åº¦çˆ¬å–
            if depth > 1:
                child_results = {}
                # é™åˆ¶çˆ¬å–é“¾æ¥æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
                for child_url in links[:min(10, len(links))]:
                    # åªçˆ¬å–åŒåŸŸåä¸‹çš„é“¾æ¥
                    if self._is_same_domain(url, child_url) and child_url not in self.visited_urls:
                        child_result = self.crawl(child_url, depth - 1)
                        if child_result:
                            for k, v in child_result.items():
                                v['depth'] = result[url]['depth'] + 1
                                child_results[k] = v
                
                result.update(child_results)
                
        return result
        
    def batch_crawl(self, urls, depth=1):
        """
        æ‰¹é‡çˆ¬å–å¤šä¸ªURL
        
        å‚æ•°:
            urls: URLåˆ—è¡¨
            depth: çˆ¬å–æ·±åº¦
            
        è¿”å›:
            çˆ¬å–ç»“æœå­—å…¸
        """
        results = {}
        futures = []
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        for url in urls:
            if url not in self.visited_urls:
                futures.append(self.executor.submit(self.crawl, url, depth))
        
        # æ”¶é›†ç»“æœ
        for future in futures:
            try:
                result = future.result()
                results.update(result)
            except Exception as e:
                logger.error(f"çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")
                
        return results
    
    def _is_same_domain(self, url1, url2):
        """æ£€æŸ¥ä¸¤ä¸ªURLæ˜¯å¦å±äºåŒä¸€åŸŸå"""
        domain1 = urlparse(url1).netloc
        domain2 = urlparse(url2).netloc
        return domain1 == domain2
        
    def close(self):
        """å…³é—­èµ„æº"""
        self.executor.shutdown(wait=True)
        self.session.close()


def extract_embedded_media(html_content, base_url=None):
    """
    ä»HTMLå†…å®¹ä¸­æå–å†…åµŒçš„åª’ä½“å†…å®¹ï¼ˆYouTubeè§†é¢‘ã€Twitterå¸–å­ç­‰ï¼‰
    
    å‚æ•°:
        html_content: HTMLå†…å®¹
        base_url: åŸºç¡€URLï¼Œç”¨äºè§£æç›¸å¯¹é“¾æ¥
        
    è¿”å›:
        åª’ä½“å†…å®¹å­—å…¸ï¼Œæ ¼å¼ä¸º {ç±»å‹: [é“¾æ¥åˆ—è¡¨]}
    """
    if not html_content:
        return {}
        
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        media = {
            "youtube": [],
            "twitter": [],
            "vimeo": [],
            "instagram": [],
            "other_embedded": []
        }
        
        # æå–YouTubeè§†é¢‘ (iframeæ–¹å¼)
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src', '')
            if not src:
                continue
                
            # è½¬æ¢ç›¸å¯¹URLä¸ºç»å¯¹URL
            if base_url and not (src.startswith('http://') or src.startswith('https://')):
                src = urljoin(base_url, src)
                
            # æ£€æµ‹YouTube
            if 'youtube.com/embed/' in src or 'youtube-nocookie.com/embed/' in src:
                media["youtube"].append(src)
            # æ£€æµ‹Vimeo
            elif 'player.vimeo.com/video/' in src:
                media["vimeo"].append(src)
            # å…¶ä»–iframeåµŒå…¥
            else:
                media["other_embedded"].append(src)
        
        # æå–Twitterå¸–å­ (å¸¸è§çš„åµŒå…¥æ–¹å¼)
        for div in soup.find_all('div', class_=lambda c: c and ('twitter-tweet' in c or 'twitter-timeline' in c)):
            # å¯»æ‰¾Twitteré“¾æ¥
            for a in div.find_all('a'):
                href = a.get('href', '')
                if 'twitter.com' in href and not href in media["twitter"]:
                    media["twitter"].append(href)
        
        # æå–Twitteré“¾æ¥ (å¦ä¸€ç§æ–¹å¼)
        for blockquote in soup.find_all('blockquote', class_=lambda c: c and 'twitter-tweet' in c):
            for a in blockquote.find_all('a'):
                href = a.get('href', '')
                if 'twitter.com' in href and not href in media["twitter"]:
                    media["twitter"].append(href)
        
        # æå–Instagramå¸–å­
        for blockquote in soup.find_all('blockquote', class_=lambda c: c and 'instagram-media' in c):
            for a in blockquote.find_all('a'):
                href = a.get('href', '')
                if 'instagram.com/p/' in href and not href in media["instagram"]:
                    media["instagram"].append(href)
        
        # ç§»é™¤ç©ºåˆ—è¡¨
        for k in list(media.keys()):
            if not media[k]:
                del media[k]
                
        return media
        
    except Exception as e:
        logger.error(f"æå–åª’ä½“å†…å®¹å‡ºé”™: {str(e)}")
        return {}


class DataProcessor:
    """æ•°æ®å¤„ç†ç±»ï¼Œè´Ÿè´£æ¸…æ´—å’Œåˆ†ç±»ç½‘é¡µå†…å®¹"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        
    def clean_html(self, html_content, base_url=None):
        """
        æ¸…æ´—HTMLå†…å®¹ï¼Œç§»é™¤å¯¼èˆªæ ã€å¹¿å‘Šã€é¡µè„šç­‰ï¼Œä½†ä¿ç•™åª’ä½“å†…å®¹çš„å¼•ç”¨
        
        å‚æ•°:
            html_content: åŸå§‹HTMLå†…å®¹
            base_url: HTMLå†…å®¹çš„åŸºç¡€URLï¼Œç”¨äºè½¬æ¢ç›¸å¯¹URL
            
        è¿”å›:
            æ¸…æ´—åçš„HTMLå†…å®¹
        """
        if not html_content:
            return ""
        
        # å¤„ç†ç‰¹æ®Šå†…å®¹ç±»å‹
        if isinstance(html_content, str) and (html_content.startswith("PDF_CONTENT_") or
                                             html_content.startswith("UNSUPPORTED_CONTENT_")):
            return html_content
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–å†…åµŒåª’ä½“å†…å®¹ï¼ˆåœ¨åˆ é™¤å‰ï¼‰
            media = extract_embedded_media(html_content, base_url)
            
            # ç§»é™¤å¸¸è§å™ªå£°å…ƒç´ 
            noise_tags = [
                'script', 'style', 'nav', 'footer', 'header', 'aside',
                'noscript', 'meta', 'svg'
            ]
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†åˆ é™¤iframeï¼Œä»¥ä¿ç•™è§†é¢‘åµŒå…¥
            
            for tag in noise_tags:
                for element in soup.find_all(tag):
                    element.decompose()
            
            # ç§»é™¤å¸¸è§å¹¿å‘Šå’Œå¯¼èˆªåŒºåŸŸ(åŸºäºå¸¸è§ç±»åå’ŒID)
            ad_classes = [
                'ad', 'ads', 'advertisement', 'banner', 'social',
                'sidebar', 'footer', 'header', 'nav', 'menu'
            ]
            
            for cls in ad_classes:
                for element in soup.find_all(class_=re.compile(cls, re.I)):
                    element.decompose()
                for element in soup.find_all(id=re.compile(cls, re.I)):
                    element.decompose()
            
            # å¦‚æœæä¾›äº†åŸºç¡€URLï¼Œå°†æ‰€æœ‰ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
            if base_url:
                # è½¬æ¢å›¾ç‰‡é“¾æ¥
                for img in soup.find_all('img', src=True):
                    img['src'] = urljoin(base_url, img['src'])
                
                # è½¬æ¢CSSé“¾æ¥
                for link in soup.find_all('link', href=True):
                    link['href'] = urljoin(base_url, link['href'])
                
                # è½¬æ¢è¶…é“¾æ¥
                for a in soup.find_all('a', href=True):
                    a['href'] = urljoin(base_url, a['href'])
                
                # è½¬æ¢å…¶ä»–å¯èƒ½çš„ç›¸å¯¹URLèµ„æº
                for elem in soup.find_all(src=True):
                    elem['src'] = urljoin(base_url, elem['src'])
                
                for elem in soup.find_all(href=True):
                    elem['href'] = urljoin(base_url, elem['href'])
            
            # ä¿ç•™ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile('content|article|post|body', re.I))
            
            if main_content:
                # åªä¿ç•™ä¸»è¦å†…å®¹åŒº
                clean_html = str(main_content)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ä¸»è¦å†…å®¹åŒºï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªbody
                body = soup.find('body')
                clean_html = str(body) if body else str(soup)
            
            # æ·»åŠ åª’ä½“å†…å®¹çš„å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if media:
                media_html = '<div class="embedded-media-references">\n'
                
                if "youtube" in media:
                    media_html += '<h3>å†…åµŒYouTubeè§†é¢‘ï¼š</h3>\n<ul>\n'
                    for url in media["youtube"]:
                        media_html += f'<li><a href="{url}" target="_blank">{url}</a></li>\n'
                    media_html += '</ul>\n'
                
                if "twitter" in media:
                    media_html += '<h3>å†…åµŒTwitterå†…å®¹ï¼š</h3>\n<ul>\n'
                    for url in media["twitter"]:
                        media_html += f'<li><a href="{url}" target="_blank">{url}</a></li>\n'
                    media_html += '</ul>\n'
                
                if "vimeo" in media:
                    media_html += '<h3>å†…åµŒVimeoè§†é¢‘ï¼š</h3>\n<ul>\n'
                    for url in media["vimeo"]:
                        media_html += f'<li><a href="{url}" target="_blank">{url}</a></li>\n'
                    media_html += '</ul>\n'
                
                if "instagram" in media:
                    media_html += '<h3>å†…åµŒInstagramå†…å®¹ï¼š</h3>\n<ul>\n'
                    for url in media["instagram"]:
                        media_html += f'<li><a href="{url}" target="_blank">{url}</a></li>\n'
                    media_html += '</ul>\n'
                
                if "other_embedded" in media:
                    media_html += '<h3>å…¶ä»–å†…åµŒå†…å®¹ï¼š</h3>\n<ul>\n'
                    for url in media["other_embedded"]:
                        media_html += f'<li><a href="{url}" target="_blank">{url}</a></li>\n'
                    media_html += '</ul>\n'
                
                media_html += '</div>'
                
                # å°†åª’ä½“å¼•ç”¨æ·»åŠ åˆ°æ¸…æ´—åçš„HTMLæœ«å°¾
                clean_html += media_html
            
            return clean_html
            
        except Exception as e:
            logger.error(f"æ¸…æ´—HTMLå‡ºé”™: {str(e)}")
            return html_content
    
    def extract_text_from_html(self, html_content):
        """
        ä»HTMLæå–çº¯æ–‡æœ¬
        
        å‚æ•°:
            html_content: HTMLå†…å®¹
            
        è¿”å›:
            æå–çš„çº¯æ–‡æœ¬
        """
        if not html_content:
            return ""
            
        # å¤„ç†ç‰¹æ®Šå†…å®¹ç±»å‹
        if isinstance(html_content, str) and (html_content.startswith("PDF_CONTENT_") or
                                             html_content.startswith("UNSUPPORTED_CONTENT_")):
            if html_content.startswith("PDF_CONTENT_"):
                url = html_content.replace("PDF_CONTENT_", "")
                return f"è¿™æ˜¯ä¸€ä¸ªPDFæ–‡ä»¶ï¼Œæ— æ³•ç›´æ¥æ˜¾ç¤ºå†…å®¹ã€‚ä¸‹è½½é“¾æ¥: {url}"
            else:
                content_type = html_content.replace("UNSUPPORTED_CONTENT_", "")
                return f"è¿™æ˜¯ä¸€ä¸ª{content_type}ç±»å‹çš„æ–‡ä»¶ï¼Œæ— æ³•ç›´æ¥æ˜¾ç¤ºå†…å®¹ã€‚"
            
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ‰€æœ‰æ–‡æœ¬ï¼Œä¿ç•™åŸºæœ¬æ ¼å¼
            text_parts = []
            
            # å¤„ç†æ ‡é¢˜
            for i in range(1, 7):
                for heading in soup.find_all(f'h{i}'):
                    text = heading.get_text(strip=True)
                    if text:
                        text_parts.append(f"{'#' * i} {text}\n")
            
            # å¤„ç†æ®µè½
            for p in soup.find_all('p'):
                text = p.get_text(strip=True)
                if text:
                    text_parts.append(f"{text}\n")
            
            # å¤„ç†åˆ—è¡¨
            for ul in soup.find_all('ul'):
                for li in ul.find_all('li'):
                    text = li.get_text(strip=True)
                    if text:
                        text_parts.append(f"- {text}\n")
            
            for ol in soup.find_all('ol'):
                for i, li in enumerate(ol.find_all('li'), 1):
                    text = li.get_text(strip=True)
                    if text:
                        text_parts.append(f"{i}. {text}\n")
            
            # å¦‚æœä¸Šé¢çš„æå–ç»“æœä¸ºç©ºï¼Œåˆ™æå–æ‰€æœ‰æ–‡æœ¬
            if not text_parts:
                text = soup.get_text(separator='\n', strip=True)
                text_parts = [text]
            
            return '\n'.join(text_parts)
            
        except Exception as e:
            logger.error(f"æå–æ–‡æœ¬å‡ºé”™: {str(e)}")
            return ""
    
    def clean_text(self, text):
        """
        æ¸…æ´—æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·ç­‰
        
        å‚æ•°:
            text: åŸå§‹æ–‡æœ¬
            
        è¿”å›:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""
            
        try:
            # è½¬æ¢ä¸ºå°å†™
            text = text.lower()
            
            # ç§»é™¤URL
            text = re.sub(r'https?://\S+|www\.\S+', '', text)
            
            # ç§»é™¤HTMLæ ‡ç­¾æ®‹ç•™
            text = re.sub(r'<.*?>', '', text)
            
            # ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦ï¼ˆä¿ç•™ç©ºæ ¼å’Œæ¢è¡Œï¼‰
            text = re.sub(r'[^\w\s\n]', '', text)
            
            # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text
            
        except Exception as e:
            logger.error(f"æ¸…æ´—æ–‡æœ¬å‡ºé”™: {str(e)}")
            return text
    
    def preprocess_text(self, text):
        """
        æ–‡æœ¬é¢„å¤„ç†ï¼ˆç”¨äºåˆ†ç±»ï¼‰
        
        å‚æ•°:
            text: åŸå§‹æ–‡æœ¬
            
        è¿”å›:
            é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
            
        try:
            # æ¸…æ´—æ–‡æœ¬
            text = self.clean_text(text)
            
            # å°è¯•ä½¿ç”¨nltkè¿›è¡Œåˆ†è¯
            try:
                tokens = word_tokenize(text)
                
                # ç§»é™¤åœç”¨è¯å’ŒçŸ­å•è¯
                tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]
                
                # è¯å½¢è¿˜åŸ
                tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
                
                # é‡æ–°ç»„åˆä¸ºæ–‡æœ¬
                preprocessed_text = ' '.join(tokens)
                
                return preprocessed_text
            except LookupError as e:
                # å¦‚æœç¼ºå°‘NLTKèµ„æºï¼Œä½¿ç”¨ç®€å•çš„ç©ºæ ¼åˆ†è¯ä½œä¸ºå›é€€
                logger.warning(f"NLTKèµ„æºç¼ºå¤±ï¼Œä½¿ç”¨ç®€å•åˆ†è¯: {str(e)}")
                
                # ç®€å•åˆ†è¯
                tokens = text.split()
                
                # ç§»é™¤çŸ­å•è¯
                tokens = [token for token in tokens if len(token) > 2]
                
                return ' '.join(tokens)
                
        except Exception as e:
            logger.error(f"é¢„å¤„ç†æ–‡æœ¬å‡ºé”™: {str(e)}")
            # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºå›é€€
            return text
    
    def extract_keywords(self, text, top_n=10):
        """
        æå–æ–‡æœ¬å…³é”®è¯
        
        å‚æ•°:
            text: æ–‡æœ¬å†…å®¹
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
            
        è¿”å›:
            å…³é”®è¯åˆ—è¡¨
        """
        if not text:
            return []
            
        try:
            # å°è¯•ä½¿ç”¨NLTK
            try:
                # é¢„å¤„ç†æ–‡æœ¬
                preprocessed_text = self.preprocess_text(text)
                
                # åˆ†è¯
                tokens = word_tokenize(preprocessed_text)
                
                # è®¡ç®—è¯é¢‘
                word_freq = {}
                for token in tokens:
                    if token in word_freq:
                        word_freq[token] += 1
                    else:
                        word_freq[token] = 1
                
                # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„è¯ä½œä¸ºå…³é”®è¯
                keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
                
                return [keyword for keyword, _ in keywords]
                
            except LookupError as e:
                # å¦‚æœç¼ºå°‘NLTKèµ„æºï¼Œä½¿ç”¨ç®€å•çš„æ–¹æ³•æå–å…³é”®è¯
                logger.warning(f"NLTKèµ„æºç¼ºå¤±ï¼Œä½¿ç”¨ç®€å•å…³é”®è¯æå–: {str(e)}")
                
                # ç®€å•åˆ†è¯
                tokens = text.lower().split()
                
                # ç§»é™¤çŸ­å•è¯
                tokens = [token for token in tokens if len(token) > 2]
                
                # è®¡ç®—è¯é¢‘
                word_freq = {}
                for token in tokens:
                    if token in word_freq:
                        word_freq[token] += 1
                    else:
                        word_freq[token] = 1
                
                # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„è¯ä½œä¸ºå…³é”®è¯
                keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
                
                return [keyword for keyword, _ in keywords]
                
        except Exception as e:
            logger.error(f"æå–å…³é”®è¯å‡ºé”™: {str(e)}")
            return []
    
    def classify_content(self, content_list, n_clusters=5):
        """
        å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»
        
        å‚æ•°:
            content_list: å†…å®¹åˆ—è¡¨
            n_clusters: åˆ†ç±»æ•°é‡
            
        è¿”å›:
            åˆ†ç±»ç»“æœå­—å…¸ï¼Œé”®ä¸ºåˆ†ç±»IDï¼Œå€¼ä¸ºè¯¥åˆ†ç±»ä¸­çš„å†…å®¹ç´¢å¼•åˆ—è¡¨
        """
        if not content_list or len(content_list) < n_clusters:
            return {0: list(range(len(content_list)))}
            
        try:
            # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
            try:
                preprocessed_texts = [self.preprocess_text(text) for text in content_list]
                
                # ä½¿ç”¨TF-IDFå‘é‡åŒ–æ–‡æœ¬
                vectorizer = TfidfVectorizer(max_features=1000)
                X = vectorizer.fit_transform(preprocessed_texts)
                
                # ä½¿ç”¨K-Meansèšç±»
                kmeans = KMeans(n_clusters=min(n_clusters, len(content_list)), random_state=42)
                kmeans.fit(X)
                
                # è·å–èšç±»ç»“æœ
                labels = kmeans.labels_
                
                # æ•´ç†åˆ†ç±»ç»“æœ
                clusters = {}
                for i, label in enumerate(labels):
                    label_int = int(label)  # ç¡®ä¿æ ‡ç­¾æ˜¯PythonåŸç”Ÿæ•´æ•°
                    if label_int not in clusters:
                        clusters[label_int] = []
                    clusters[label_int].append(i)
                    
                return clusters
                
            except LookupError as e:
                # NLTKé—®é¢˜ï¼Œä½¿ç”¨ç®€å•åˆ†ç±»
                logger.warning(f"NLTKç›¸å…³é”™è¯¯ï¼Œä½¿ç”¨ç®€å•åˆ†ç±»: {str(e)}")
                
                # å¹³å‡åˆ†é…åˆ°n_clustersç»„
                n = len(content_list)
                actual_clusters = min(n_clusters, n)
                
                # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œå°±åˆ†æˆä¸€ç»„
                if n <= actual_clusters:
                    return {0: list(range(n))}
                    
                # ç®€å•åˆ†ç»„ - å¹³å‡åˆ†é…
                clusters = {}
                items_per_cluster = n // actual_clusters
                remainder = n % actual_clusters
                
                start = 0
                for i in range(actual_clusters):
                    # ä¸ºå‰remainderä¸ªç»„å¤šåˆ†é…ä¸€ä¸ªå…ƒç´ 
                    count = items_per_cluster + (1 if i < remainder else 0)
                    clusters[i] = list(range(start, start + count))
                    start += count
                    
                return clusters
                
        except Exception as e:
            logger.error(f"åˆ†ç±»å†…å®¹å‡ºé”™: {str(e)}")
            # è¿”å›å•ä¸€åˆ†ç±»ä½œä¸ºå›é€€
            return {0: list(range(len(content_list)))}
    
    def format_content(self, content, format_type='txt'):
        """
        æ ¼å¼åŒ–å†…å®¹
        
        å‚æ•°:
            content: å†…å®¹æ–‡æœ¬
            format_type: æ ¼å¼ç±»å‹ï¼Œ'txt'æˆ–'html'
            
        è¿”å›:
            æ ¼å¼åŒ–åçš„å†…å®¹
        """
        if not content:
            return ""
            
        # å¤„ç†ç‰¹æ®Šå†…å®¹ç±»å‹
        if isinstance(content, str) and (content.startswith("PDF_CONTENT_") or
                                        content.startswith("UNSUPPORTED_CONTENT_")):
            if content.startswith("PDF_CONTENT_"):
                url = content.replace("PDF_CONTENT_", "")
                return f"è¿™æ˜¯ä¸€ä¸ªPDFæ–‡ä»¶ï¼Œæ— æ³•ç›´æ¥æ˜¾ç¤ºå†…å®¹ã€‚ä¸‹è½½é“¾æ¥: {url}"
            else:
                content_type = content.replace("UNSUPPORTED_CONTENT_", "")
                return f"è¿™æ˜¯ä¸€ä¸ª{content_type}ç±»å‹çš„æ–‡ä»¶ï¼Œæ— æ³•ç›´æ¥æ˜¾ç¤ºå†…å®¹ã€‚"
            
        if format_type.lower() == 'txt':
            # ç¡®ä¿çº¯æ–‡æœ¬æ ¼å¼
            return self.extract_text_from_html(content) if '<' in content and '>' in content else content
            
        elif format_type.lower() == 'html':
            # å¦‚æœå†…å®¹å·²ç»æ˜¯HTMLï¼Œæ¸…æ´—å®ƒ
            if '<' in content and '>' in content:
                return self.clean_html(content)
            
            # å¦‚æœæ˜¯çº¯æ–‡æœ¬ï¼Œè½¬ä¸ºHTML
            else:
                html_parts = ['<!DOCTYPE html>', '<html>', '<head><meta charset="UTF-8"></head>', '<body>']
                
                # åˆ†æ®µå¤„ç†
                paragraphs = content.split('\n\n')
                for p in paragraphs:
                    if p.strip():
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜ï¼ˆä»¥#å¼€å¤´ï¼‰
                        if p.strip().startswith('#'):
                            level = 0
                            for char in p:
                                if char == '#':
                                    level += 1
                                else:
                                    break
                            level = min(level, 6)  # h1-h6
                            title_text = p[level:].strip()
                            html_parts.append(f'<h{level}>{title_text}</h{level}>')
                        else:
                            html_parts.append(f'<p>{p.strip()}</p>')
                
                html_parts.extend(['</body>', '</html>'])
                return '\n'.join(html_parts)
                
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹: {format_type}")
            return content


class StorageManager:
    """å­˜å‚¨ç®¡ç†ç±»ï¼Œè´Ÿè´£ä¿å­˜çˆ¬å–ç»“æœ"""
    
    def __init__(self, base_dir='./crawled_data'):
        """
        åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
        
        å‚æ•°:
            base_dir: åŸºç¡€å­˜å‚¨ç›®å½•
        """
        self.base_dir = base_dir
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.run_dir = os.path.join(base_dir, f"run_{timestamp}")
        self._ensure_base_dir()
        
    def _ensure_base_dir(self):
        """ç¡®ä¿åŸºç¡€ç›®å½•å’Œè¿è¡Œç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.base_dir):
            os.makedirs(self.base_dir)
            logger.info(f"åˆ›å»ºåŸºç¡€ç›®å½•: {self.base_dir}")
            
        # ç¡®ä¿è¿è¡Œå­ç›®å½•å­˜åœ¨
        if not os.path.exists(self.run_dir):
            os.makedirs(self.run_dir)
            logger.info(f"åˆ›å»ºè¿è¡Œç›®å½•: {self.run_dir}")
            
    def _get_domain_dir(self, url):
        """
        è·å–URLå¯¹åº”çš„åŸŸåç›®å½•
        
        å‚æ•°:
            url: ç½‘é¡µURL
            
        è¿”å›:
            åŸŸåç›®å½•è·¯å¾„
        """
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        
        # åˆ›å»ºåŸŸåç›®å½• (åœ¨è¿è¡Œå­ç›®å½•ä¸‹)
        domain_dir = os.path.join(self.run_dir, domain)
        if not os.path.exists(domain_dir):
            os.makedirs(domain_dir)
            
        return domain_dir
        
    def _get_url_filename(self, url, format_type):
        """
        ç”ŸæˆURLå¯¹åº”çš„æ–‡ä»¶å
        
        å‚æ•°:
            url: ç½‘é¡µURL
            format_type: æ–‡ä»¶æ ¼å¼ï¼ˆtxtæˆ–htmlï¼‰
            
        è¿”å›:
            å®Œæ•´æ–‡ä»¶è·¯å¾„
        """
        # æå–URLè·¯å¾„éƒ¨åˆ†
        parsed_url = urlparse(url)
        path = parsed_url.path.strip('/')
        
        # å¤„ç†è·¯å¾„ä¸ºç©ºçš„æƒ…å†µï¼ˆç½‘ç«™é¦–é¡µï¼‰
        if not path:
            path = 'index'
        
        # å¤„ç†è¿‡é•¿çš„è·¯å¾„
        if len(path) > 50:
            # ä½¿ç”¨å“ˆå¸Œå€¼ç¼©çŸ­æ–‡ä»¶å
            path_hash = hashlib.md5(path.encode()).hexdigest()[:10]
            path = f"{path[:30]}_{path_hash}"
            
        # æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
        path = path.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_')
        path = path.replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_')
        path = path.replace('|', '_')
        
        # ç”Ÿæˆæ–‡ä»¶å (ä¸å†éœ€è¦æ—¶é—´æˆ³ï¼Œå› ä¸ºå·²ç»åœ¨ç›®å½•ä¸­åŒ…å«äº†)
        filename = f"{path}.{format_type}"
        
        # è·å–åŸŸåç›®å½•
        domain_dir = self._get_domain_dir(url)
        
        return os.path.join(domain_dir, filename)
    
    def save_content(self, url, content, format_type='txt', metadata=None):
        """
        ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
        
        å‚æ•°:
            url: ç½‘é¡µURL
            content: ç½‘é¡µå†…å®¹
            format_type: æ–‡ä»¶æ ¼å¼ï¼ˆtxtæˆ–htmlï¼‰
            metadata: å…ƒæ•°æ®å­—å…¸
            
        è¿”å›:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not content:
            logger.warning(f"å†…å®¹ä¸ºç©ºï¼Œä¸ä¿å­˜: {url}")
            return None
            
        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            file_path = self._get_url_filename(url, format_type)
            
            # å†™å…¥å†…å®¹
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
            logger.info(f"ä¿å­˜å†…å®¹åˆ°: {file_path}")
            
            # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œä¿å­˜å…ƒæ•°æ®
            if metadata:
                meta_path = f"{os.path.splitext(file_path)[0]}.meta.json"
                with open(meta_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                logger.info(f"ä¿å­˜å…ƒæ•°æ®åˆ°: {meta_path}")
                
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜å†…å®¹å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            return None
    
    def save_batch_content(self, content_dict, format_type='txt'):
        """
        æ‰¹é‡ä¿å­˜å†…å®¹
        
        å‚æ•°:
            content_dict: URLåˆ°å†…å®¹çš„å­—å…¸æ˜ å°„
            format_type: æ–‡ä»¶æ ¼å¼ï¼ˆtxtæˆ–htmlï¼‰
            
        è¿”å›:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        saved_paths = []
        
        for url, data in content_dict.items():
            content = data.get('content', '')
            metadata = {
                'title': data.get('title', ''),
                'url': url,
                'crawl_time': datetime.now().isoformat(),
                'depth': data.get('depth', 0),
                'links': data.get('links', [])
            }
            
            path = self.save_content(url, content, format_type, metadata)
            if path:
                saved_paths.append(path)
                
        return saved_paths
        
    def get_run_directory(self):
        """
        è·å–å½“å‰è¿è¡Œçš„ç›®å½•è·¯å¾„
        
        è¿”å›:
            å½“å‰è¿è¡Œçš„ç›®å½•è·¯å¾„
        """
        return self.run_dir

# ---------------------------
# éƒ½å¸‚ä¼ è¯´åˆ¤æ–­ç®—æ³•æ¨¡å—
# ---------------------------
class UrbanLegendAnalyzer:
    """éƒ½å¸‚ä¼ è¯´åˆ†æå™¨ï¼Œç”¨äºæ£€æµ‹å’Œè¯„ä¼°çˆ¬å–å†…å®¹ä¸­çš„éƒ½å¸‚ä¼ è¯´"""
    
    def __init__(self):
        """åˆå§‹åŒ–éƒ½å¸‚ä¼ è¯´åˆ†æå™¨"""
        # é»˜è®¤é˜ˆå€¼
        self.thresholds = {
            'confirmed_J': 1.8,
            'suspect_J': 1.3,
            'lag': 2.0,
            # å†…å®¹ç‰¹å¾é˜ˆå€¼
            'keyword_score': 0.7,
            'credibility_score': 0.5
        }
    
    def urban_legend_judge_seo(self, 
                               theta_values, 
                               y_values, 
                               time_array, 
                               seo_factor, 
                               alpha_seo=0.5, 
                               thresholds=None):
        """
        è€ƒè™‘ SEO å› ç´ çš„éƒ½å¸‚ä¼ è¯´è¯†åˆ«å‡½æ•°
        
        å‚æ•°ï¼š
        ----------
        theta_values : 1D array-like
            è®®é¢˜çƒ­åº¦åºåˆ—
        y_values : 1D array-like
            ååˆ¶ä¿¡æ¯å¼ºåº¦åºåˆ—
        time_array : 1D array-like
            æ—¶é—´åºåˆ—ï¼Œä¸ä¸Šé¢ä¸¤ä¸ªåºåˆ—ä¸€ä¸€å¯¹åº”
        seo_factor : float
            SEO å› å­ï¼ˆè¶Šå¤§è¶Šè¯´æ˜å¸–å­åœ¨æœç´¢å¼•æ“ä¸­çš„æ’åæˆ–æ›å…‰è¶Šé«˜ï¼‰
        alpha_seo : float
            SEO æ”¾å¤§ç³»æ•°ï¼Œç”¨äºå†³å®š SEO å¯¹æœ€ç»ˆ J å€¼çš„å½±å“
        thresholds : dict
            åŒ…å« 'confirmed_J', 'suspect_J', 'lag' ç­‰åˆ¤å®šé˜ˆå€¼
        
        è¿”å›ï¼š
        ----------
        label : str
            "âœ… å·²ç¡®è®¤éƒ½å¸‚ä¼ è¯´" / "âš ï¸ ç–‘ä¼¼éƒ½å¸‚ä¼ è¯´" / "ğŸŸ¢ æ™®é€šå¸–å­"
        details : dict
            å…·ä½“è®¡ç®—ç»“æœï¼Œç”¨äºåç»­åˆ†æ
        """

        if thresholds is None:
            thresholds = self.thresholds

        theta_max = np.max(theta_values)
        y_max = np.max(y_values)
        epsilon = 1e-6
        J = theta_max / (y_max + epsilon)
        # SEO ä¿®æ­£åçš„ä¼ æ’­æŒ‡æ•°
        J_SEO = J * (1 + alpha_seo * seo_factor)

        # è®¡ç®—è®®é¢˜çƒ­åº¦çš„å±€éƒ¨å³°å€¼æ•°é‡ï¼ˆç®€å•æ–¹æ³•ï¼‰
        num_peaks_theta = 0
        for i in range(1, len(theta_values) - 1):
            if theta_values[i] > theta_values[i-1] and theta_values[i] > theta_values[i+1]:
                num_peaks_theta += 1

        # è®¡ç®—ååˆ¶å»¶è¿Ÿï¼šå–æœ€å¤§å€¼å¯¹åº”æ—¶é—´çš„å·®å€¼
        t_theta_max = time_array[np.argmax(theta_values)]
        t_y_max = time_array[np.argmax(y_values)]
        lag_time = abs(t_y_max - t_theta_max)

        # åˆ¤æ–­é€»è¾‘
        label = "ğŸŸ¢ æ™®é€šå¸–å­"
        if J_SEO > thresholds['confirmed_J'] and lag_time > thresholds['lag'] and num_peaks_theta >= 2:
            label = "âœ… å·²ç¡®è®¤éƒ½å¸‚ä¼ è¯´"
        elif J_SEO > thresholds['suspect_J'] and lag_time > thresholds['lag']:
            label = "âš ï¸ ç–‘ä¼¼éƒ½å¸‚ä¼ è¯´"

        details = {
            'theta_max': theta_max,
            'y_max': y_max,
            'J_raw': J,
            'J_SEO': J_SEO,
            'seo_factor': seo_factor,
            'num_peaks_theta': num_peaks_theta,
            'lag_time': lag_time,
            'thresholds': thresholds
        }

        return label, details
    
    def analyze_content(self, content, url, metadata=None):
        """
        åˆ†æçˆ¬å–çš„å†…å®¹æ˜¯å¦ä¸ºéƒ½å¸‚ä¼ è¯´
        
        å‚æ•°:
            content: ç½‘é¡µå†…å®¹æ–‡æœ¬
            url: ç½‘é¡µURL
            metadata: å…ƒæ•°æ®ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        if not content:
            return {
                'label': "ğŸ”„ æ— æ³•åˆ†æ",
                'details': {"reason": "å†…å®¹ä¸ºç©º"},
                'url': url,
                'analysis_time': datetime.now().isoformat()
            }
            
        try:
            # è¿™é‡Œæ˜¯ç®€åŒ–çš„åˆ†æè¿‡ç¨‹ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å†…å®¹ç‰¹å¾æå–çƒ­åº¦å’Œååˆ¶ä¿¡æ¯
            
            # åŸºäºå†…å®¹é•¿åº¦å’Œå¤æ‚åº¦çš„ç®€å•ä¼°è®¡
            content_length = len(content)
            # å­—ç¬¦æ•° / 1000 ä½œä¸ºç®€å•çš„SEOå› å­ï¼ˆè¶Šé•¿çš„æ–‡ç« å¯èƒ½è¶Šå—æ¬¢è¿ï¼‰
            seo_factor = min(5.0, content_length / 1000)
            
            # ç®€å•æ¨¡æ‹Ÿçƒ­åº¦å’Œååˆ¶åºåˆ—
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›åº”è¯¥ä»å†å²æ•°æ®ä¸­æå–
            t = np.linspace(0, 20, 100)
            
            # ä½¿ç”¨å†…å®¹æŒ‡æ ‡æ„é€ åˆæˆçš„çƒ­åº¦æ›²çº¿
            # è¿™é‡Œé‡‡ç”¨éšæœºå€¼+å†…å®¹é•¿åº¦å› ç´ ä½œä¸ºæ¼”ç¤º
            # å®é™…åº”ç”¨åº”è¯¥åŸºäºçœŸå®çš„ä¼ æ’­æ•°æ®
            noise = np.random.normal(0, 0.1, len(t))
            theta_t = 1.0 + 0.5 * np.sin(t/2) + 0.3 * np.sin(t) + noise + (content_length / 50000)
            
            # ååˆ¶ä¿¡æ¯æ›²çº¿ï¼Œé€šå¸¸ç¨å¾®æ»åäºçƒ­åº¦æ›²çº¿
            y_t = 0.8 + 0.4 * np.sin((t-1.5)/2) + 0.2 * np.sin(t-1.5) + np.random.normal(0, 0.1, len(t))
            
            # æå–å…³é”®è¯ç‰¹å¾
            # ç®€å•ç¤ºä¾‹ï¼šæ£€æŸ¥ä¸€äº›å¸¸è§çš„éƒ½å¸‚ä¼ è¯´å…³é”®è¯
            urban_legend_keywords = [
                "éœ‡æƒŠ", "ç§˜å¯†", "ä¸ä¸ºäººçŸ¥", "æ­ç§˜", "æƒŠäººçœŸç›¸", "å®˜æ–¹æ©ç›–", 
                "ä¸ä¼šå‘Šè¯‰ä½ ", "åŒ»ç”Ÿä¸ä¼šå‘Šè¯‰ä½ ", "æ”¿åºœéšç’", "é˜´è°‹è®º",
                "ç»å¯†", "è½¬å‘", "æ‰©æ•£", "æ³¨æ„"
            ]
            
            keyword_count = sum(1 for keyword in urban_legend_keywords if keyword in content)
            keyword_score = min(1.0, keyword_count / 5)  # æœ€å¤š5ä¸ªå…³é”®è¯å°±ç®—æ»¡åˆ†
            
            # è°ƒæ•´SEOå› å­åŠ å…¥å…³é”®è¯å¾—åˆ†
            seo_factor = seo_factor * (1 + keyword_score)
            
            # è°ƒç”¨éƒ½å¸‚ä¼ è¯´åˆ¤æ–­å‡½æ•°
            label, details = self.urban_legend_judge_seo(theta_t, y_t, t, seo_factor)
            
            # æ·»åŠ é¢å¤–çš„å†…å®¹åˆ†ææŒ‡æ ‡
            details.update({
                'content_length': content_length,
                'keyword_score': keyword_score,
                'keyword_count': keyword_count,
                'matched_keywords': [kw for kw in urban_legend_keywords if kw in content]
            })
            
            # æ„é€ æœ€ç»ˆç»“æœ
            result = {
                'label': label,
                'details': details,
                'url': url,
                'analysis_time': datetime.now().isoformat()
            }
            
            # å¦‚æœæœ‰å…ƒæ•°æ®ï¼ŒåŠ å…¥ä¸€äº›å…³é”®å…ƒæ•°æ®
            if metadata:
                result['metadata'] = {
                    'title': metadata.get('title', ''),
                    'keywords': metadata.get('keywords', []),
                    'crawl_time': metadata.get('crawl_time', '')
                }
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æå†…å®¹å‡ºé”™: {url}, é”™è¯¯: {str(e)}")
            return {
                'label': "ğŸ”„ åˆ†æå¤±è´¥",
                'details': {"error": str(e)},
                'url': url,
                'analysis_time': datetime.now().isoformat()
            }
    
    def batch_analyze(self, content_list):
        """
        æ‰¹é‡åˆ†æå¤šä¸ªå†…å®¹
        
        å‚æ•°:
            content_list: å†…å®¹åˆ—è¡¨ï¼Œæ¯é¡¹åº”åŒ…å«contentå’Œurl
            
        è¿”å›:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        
        for item in content_list:
            content = item.get('content', '')
            url = item.get('url', '')
            metadata = item.get('metadata', None)
            
            result = self.analyze_content(content, url, metadata)
            results.append(result)
            
        return results

class APIConnector:
    """ä¸åç«¯APIè¿æ¥çš„ç±»ï¼Œç”¨äºä¸Šä¼ çˆ¬è™«ç»“æœå’Œè·å–é…ç½®"""
    
    def __init__(self, api_base_url=None, api_key=None):
        """
        åˆå§‹åŒ–APIè¿æ¥å™¨
        
        å‚æ•°:
            api_base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
        """
        # é»˜è®¤APIåœ°å€
        self.api_base_url = api_base_url or "https://api.holograplaplace.com"
        self.api_key = api_key
        self.session = requests.Session()
        
        # å¦‚æœæœ‰APIå¯†é’¥ï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´
        if self.api_key:
            self.session.headers.update({
                "Authorization": f"Bearer {self.api_key}"
            })
        
        # è®¾ç½®é€šç”¨è¯·æ±‚å¤´
        self.session.headers.update({
            "Content-Type": "application/json",
            "User-Agent": "HologramLaplace-Crawler/1.0"
        })
    
    def get_configuration(self, config_id=None):
        """
        ä»æœåŠ¡å™¨è·å–çˆ¬è™«é…ç½®
        
        å‚æ•°:
            config_id: é…ç½®IDï¼ˆå¯é€‰ï¼‰
            
        è¿”å›:
            é…ç½®å­—å…¸
        """
        try:
            # å¦‚æœæä¾›äº†é…ç½®IDï¼Œè·å–ç‰¹å®šé…ç½®
            if config_id:
                response = self.session.get(f"{self.api_base_url}/api/crawler/config/{config_id}")
            else:
                # å¦åˆ™è·å–é»˜è®¤é…ç½®
                response = self.session.get(f"{self.api_base_url}/api/crawler/config/default")
                
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–é…ç½®å¤±è´¥: {str(e)}")
            # è¿”å›åŸºæœ¬é…ç½®ä½œä¸ºå›é€€
            return {
                "urls": [],
                "depth": 2,
                "format": "html",
                "concurrency": 3
            }
    
    def upload_results(self, results, task_id=None):
        """
        ä¸Šä¼ çˆ¬å–ç»“æœåˆ°æœåŠ¡å™¨
        
        å‚æ•°:
            results: çˆ¬å–ç»“æœå­—å…¸
            task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
            
        è¿”å›:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        try:
            # å‡†å¤‡ä¸Šä¼ æ•°æ®
            payload = {
                "results": results,
                "task_id": task_id,
                "timestamp": datetime.now().isoformat(),
                "client_info": {
                    "version": "1.0",
                    "platform": platform.system(),
                    "python_version": platform.python_version()
                }
            }
            
            # å‘é€POSTè¯·æ±‚
            response = self.session.post(
                f"{self.api_base_url}/api/crawler/results",
                json=payload
            )
            
            response.raise_for_status()
            result = response.json()
            
            logger.info(f"ç»“æœä¸Šä¼ æˆåŠŸï¼ŒæœåŠ¡å™¨è¿”å›: {result}")
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ä¸Šä¼ ç»“æœå¤±è´¥: {str(e)}")
            return False
    
    def get_media_info(self, media_url):
        """
        è·å–åª’ä½“å†…å®¹çš„é¢å¤–ä¿¡æ¯
        
        å‚æ•°:
            media_url: åª’ä½“URLï¼ˆYouTubeã€Twitterç­‰ï¼‰
            
        è¿”å›:
            åª’ä½“ä¿¡æ¯å­—å…¸
        """
        try:
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            payload = {
                "url": media_url
            }
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                f"{self.api_base_url}/api/media/info",
                json=payload
            )
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–åª’ä½“ä¿¡æ¯å¤±è´¥: {str(e)}")
            # è¿”å›åŸºæœ¬ä¿¡æ¯ä½œä¸ºå›é€€
            media_type = "unknown"
            if "youtube.com" in media_url or "youtu.be" in media_url:
                media_type = "youtube"
            elif "twitter.com" in media_url:
                media_type = "twitter"
            elif "vimeo.com" in media_url:
                media_type = "vimeo"
            elif "instagram.com" in media_url:
                media_type = "instagram"
                
            return {
                "url": media_url,
                "type": media_type,
                "title": None,
                "thumbnail": None,
                "author": None,
                "publish_date": None
            }
    
    def report_error(self, error_info):
        """
        å‘æœåŠ¡å™¨æŠ¥å‘Šé”™è¯¯
        
        å‚æ•°:
            error_info: é”™è¯¯ä¿¡æ¯å­—å…¸
            
        è¿”å›:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        try:
            # å‡†å¤‡é”™è¯¯ä¿¡æ¯
            payload = {
                "error": error_info,
                "timestamp": datetime.now().isoformat(),
                "client_info": {
                    "version": "1.0",
                    "platform": platform.system(),
                    "python_version": platform.python_version()
                }
            }
            
            # å‘é€é”™è¯¯æŠ¥å‘Š
            response = self.session.post(
                f"{self.api_base_url}/api/crawler/error",
                json=payload
            )
            
            response.raise_for_status()
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"æŠ¥å‘Šé”™è¯¯å¤±è´¥: {str(e)}")
            return False


# åœ¨mainå‡½æ•°ä¸­æ·»åŠ APIé›†æˆ
def main_with_api():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å…¨æ¯æ‹‰æ™®æ‹‰æ–¯äº’è”ç½‘çˆ¬è™«ç³»ç»Ÿ')
    parser.add_argument('-c', '--config', type=str, default='crawler_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', type=str, default='crawler_results.json', help='ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¾“å‡ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('-a', '--api', action='store_true', help='ä½¿ç”¨APIè·å–é…ç½®å¹¶ä¸Šä¼ ç»“æœ')
    parser.add_argument('-u', '--api-url', type=str, help='APIåŸºç¡€URL')
    parser.add_argument('-k', '--api-key', type=str, help='APIå¯†é’¥')
    parser.add_argument('-t', '--task-id', type=str, help='ä»»åŠ¡ID')
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # åˆå§‹åŒ–APIè¿æ¥å™¨ï¼ˆå¦‚æœå¯ç”¨äº†APIï¼‰
    api = None
    if args.api:
        api = APIConnector(api_base_url=args.api_url, api_key=args.api_key)
    
    # è·å–é…ç½®
    config = None
    if api and args.api:
        # ä»APIè·å–é…ç½®
        print("ä»APIè·å–é…ç½®...")
        config = api.get_configuration(args.task_id)
        print(f"æˆåŠŸè·å–APIé…ç½®ï¼ŒåŒ…å« {len(config.get('urls', []))} ä¸ªURL")
    else:
        # ä»æœ¬åœ°æ–‡ä»¶è·å–é…ç½®
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            print("è¯·å…ˆä»ç½‘é¡µç•Œé¢ç”Ÿæˆå¹¶ä¸‹è½½é…ç½®æ–‡ä»¶")
            return
        except json.JSONDecodeError:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {args.config}")
            print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {args.config}")
            return
    
    # æå–é…ç½®
    urls = config.get('urls', [])
    depth = config.get('depth', 2)
    format_type = config.get('format', 'html')
    concurrency = config.get('concurrency', 3)
    
    if not urls:
        logger.error("é…ç½®ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
        print("é”™è¯¯: é…ç½®ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
        return
    
    print(f"å°†çˆ¬å– {len(urls)} ä¸ªURLï¼Œæ·±åº¦ä¸º {depth}ï¼Œå­˜å‚¨æ ¼å¼ä¸º {format_type}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    task_info = {
        "start_time": start_time,
        "urls": urls,
        "depth": depth,
        "format": format_type,
        "concurrency": concurrency
    }
    
    # åˆå§‹åŒ–çˆ¬è™«ã€å¤„ç†å™¨å’Œå­˜å‚¨ç®¡ç†å™¨
    crawler = WebCrawler(max_workers=concurrency)
    processor = DataProcessor()
    storage = StorageManager(base_dir='./crawled_data')
    
    # å°†è¿è¡Œç›®å½•è®°å½•åˆ°ä»»åŠ¡ä¿¡æ¯ä¸­
    task_info["run_directory"] = storage.get_run_directory()
    
    try:
        # æ‰¹é‡çˆ¬å–
        print("å¼€å§‹çˆ¬å–...")
        all_results = crawler.batch_crawl(urls, depth)
        print(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_results)} ä¸ªé¡µé¢")
        
        # å¤„ç†ç»“æœ
        processed_content = []
        
        print("å¤„ç†çˆ¬å–å†…å®¹...")
        for url, data in all_results.items():
            # æå–å†…å®¹
            title = data.get("title")
            content = data.get("content")
            html_content = data.get("html")
            status = data.get("status")
            
            # è·³è¿‡æ— æ•ˆå†…å®¹
            if not content and not html_content:
                continue
                
            # å¤„ç†HTMLå†…å®¹
            if html_content:
                # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæˆ–å…¶ä»–ç‰¹æ®Šå†…å®¹
                if isinstance(html_content, str) and (html_content.startswith("PDF_CONTENT_") or 
                                                     html_content.startswith("UNSUPPORTED_CONTENT_")):
                    clean_content = content  # ä½¿ç”¨parse_htmlç”Ÿæˆçš„æè¿°æ€§å†…å®¹
                else:
                    # æ­£å¸¸çš„HTMLå†…å®¹ - æ³¨æ„è¿™é‡Œä¼ å…¥äº†URLä½œä¸ºbase_url
                    clean_html = processor.clean_html(html_content, url)
                    
                    # æå–æ–‡æœ¬æˆ–æ ¼å¼åŒ–HTML
                    if format_type == "txt":
                        clean_content = processor.extract_text_from_html(clean_html)
                    else:
                        clean_content = clean_html
                        
                # æå–å…³é”®è¯ï¼ˆå¯¹äºæ‰€æœ‰å†…å®¹ç±»å‹ï¼‰
                keywords = []
                if content:
                    keywords = processor.extract_keywords(content)
                
                # å­˜å‚¨å†…å®¹
                metadata = {
                    "title": title or "æ— æ ‡é¢˜",
                    "url": url,
                    "depth": data.get("depth", 0),
                    "crawl_time": datetime.now().isoformat(),
                    "keywords": keywords,
                    "content_type": "pdf" if isinstance(html_content, str) and html_content.startswith("PDF_CONTENT_") else "html"
                }
                
                # æå–åª’ä½“å†…å®¹ä¿¡æ¯
                media = {}
                if html_content and not isinstance(html_content, str) and not (html_content.startswith("PDF_CONTENT_") or html_content.startswith("UNSUPPORTED_CONTENT_")):
                    media = extract_embedded_media(html_content, url)
                    if media and api:
                        # è·å–åª’ä½“çš„é¢å¤–ä¿¡æ¯
                        for media_type, urls_list in media.items():
                            for i, media_url in enumerate(urls_list):
                                media_info = api.get_media_info(media_url)
                                # ç”¨æ›´è¯¦ç»†çš„ä¿¡æ¯æ›¿æ¢åŸå§‹URL
                                media[media_type][i] = media_info
                    
                    # å°†åª’ä½“ä¿¡æ¯æ·»åŠ åˆ°å…ƒæ•°æ®
                    if media:
                        metadata["embedded_media"] = media
                
                file_path = storage.save_content(
                    url, 
                    clean_content, 
                    format_type, 
                    metadata
                )
                
                # æ„é€ å¤„ç†åçš„å†…å®¹å¯¹è±¡
                processed_item = {
                    "url": url,
                    "title": title or "æ— æ ‡é¢˜",
                    "content": clean_content,
                    "keywords": keywords,
                    "file_path": file_path,
                    "depth": data.get("depth", 0),
                    "format": format_type,
                    "status": status,
                    "embedded_media": media if media else None
                }
                
                processed_content.append(processed_item)
        
        # å†…å®¹åˆ†ç±»
        print("å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»...")
        categorized_content = {}
        if processed_content:
            # æå–çº¯æ–‡æœ¬ç”¨äºåˆ†ç±»
            text_contents = [item.get("content", "") for item in processed_content]
            
            # ç¡®å®šåˆ†ç±»æ•°é‡
            cluster_count = min(5, max(2, len(processed_content) // 3))
            
            # æ‰§è¡Œåˆ†ç±»
            clusters = processor.classify_content(text_contents, cluster_count)
            
            # æ•´ç†åˆ†ç±»ç»“æœ
            for cluster_id, indices in clusters.items():
                category_items = [processed_content[idx] for idx in indices]
                categorized_content[str(cluster_id)] = {
                    "id": cluster_id,
                    "items": category_items
                }
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        task_info["end_time"] = end_time
        task_info["duration"] = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        statistics = calculate_statistics(all_results, processed_content, categorized_content, task_info)
        
        # æ„é€ ç»“æœ
        result = {
            "task_info": task_info,
            "content": processed_content,
            "categories": categorized_content,
            "statistics": statistics
        }
        
        # ä¿å­˜ç»“æœåˆ°è¿è¡Œç›®å½•
        result_path = os.path.join(storage.get_run_directory(), args.output)
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        # å¦å¤–åœ¨å½“å‰ç›®å½•ä¸‹ä¹Ÿä¿å­˜ä¸€ä»½ç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        # å¦‚æœå¯ç”¨äº†APIï¼Œä¸Šä¼ ç»“æœ
        if api and args.api:
            print("ä¸Šä¼ ç»“æœåˆ°API...")
            success = api.upload_results(result, args.task_id)
            if success:
                print("ç»“æœä¸Šä¼ æˆåŠŸï¼")
            else:
                print("è­¦å‘Š: ç»“æœä¸Šä¼ å¤±è´¥ï¼Œè¯·ç¨åæ‰‹åŠ¨ä¸Šä¼ æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        
        print(f"çˆ¬å–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
        print(f" - {result_path}")
        print(f" - {os.path.abspath(args.output)} (å¤åˆ¶)")
        print(f"æ•°æ®æ–‡ä»¶ä¿å­˜åœ¨: {storage.get_run_directory()}")
        print(f"æ€»å…±çˆ¬å– {len(all_results)} ä¸ªé¡µé¢ï¼ŒæˆåŠŸç‡ {statistics['successRate']}%")
        print(f"ç”¨æ—¶ {task_info['duration']:.2f} ç§’")
        
        if not (api and args.api):
            print("è¯·å°†ç»“æœæ–‡ä»¶ä¸Šä¼ åˆ°ç½‘é¡µç•Œé¢æŸ¥çœ‹è¯¦ç»†åˆ†æ")
        
    except KeyboardInterrupt:
        print("\nçˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
        # æŠ¥å‘Šä¸­æ–­é”™è¯¯
        if api and args.api:
            api.report_error({
                "type": "keyboard_interrupt",
                "message": "ç”¨æˆ·ä¸­æ–­çˆ¬å–",
                "task_id": args.task_id
            })
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        print(f"é”™è¯¯: {str(e)}")
        # æŠ¥å‘Šå…¶ä»–é”™è¯¯
        if api and args.api:
            api.report_error({
                "type": "exception",
                "message": str(e),
                "traceback": traceback.format_exc(),
                "task_id": args.task_id
            })
    finally:
        # å…³é—­èµ„æº
        crawler.close()

def calculate_statistics(all_results, processed_content, categorized_content, task_info):
    """
    è®¡ç®—ç»Ÿè®¡æ•°æ®
    
    å‚æ•°:
        all_results: æ‰€æœ‰çˆ¬å–ç»“æœ
        processed_content: å¤„ç†åçš„å†…å®¹
        categorized_content: åˆ†ç±»ç»“æœ
        task_info: ä»»åŠ¡ä¿¡æ¯
        
    è¿”å›:
        ç»Ÿè®¡æ•°æ®å­—å…¸
    """
    statistics = {
        "totalUrls": len(all_results),
        "categoriesCount": len(categorized_content),
        "successRate": round(len(processed_content) / max(1, len(all_results)) * 100, 2),
        "statusCounts": {},
        "domainCounts": {}
    }
    
    # è®¡ç®—çŠ¶æ€ç ç»Ÿè®¡
    for url, data in all_results.items():
        status = data.get("status", 0)
        status_str = str(status)  # ç¡®ä¿é”®æ˜¯å­—ç¬¦ä¸²ï¼Œé¿å…JSONåºåˆ—åŒ–é—®é¢˜
        if status_str in statistics["statusCounts"]:
            statistics["statusCounts"][status_str] += 1
        else:
            statistics["statusCounts"][status_str] = 1
    
    # è®¡ç®—åŸŸåç»Ÿè®¡
    for url in all_results.keys():
        domain = urlparse(url).netloc
        if domain in statistics["domainCounts"]:
            statistics["domainCounts"][domain] += 1
        else:
            statistics["domainCounts"][domain] = 1
    
    # è®¡ç®—å¹³å‡çˆ¬å–æ—¶é—´
    if "start_time" in task_info and "end_time" in task_info:
        total_time = task_info["end_time"] - task_info["start_time"]
        statistics["avgCrawlTime"] = round(total_time / max(1, len(processed_content)), 2)
    else:
        statistics["avgCrawlTime"] = 0
    
    return statistics


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å…¨æ¯æ‹‰æ™®æ‹‰æ–¯äº’è”ç½‘çˆ¬è™«ç³»ç»Ÿ')
    parser.add_argument('-c', '--config', type=str, default='crawler_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', type=str, default='crawler_results.json', help='ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¾“å‡ºè¯¦ç»†æ—¥å¿—')
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # è¯»å–é…ç½®æ–‡ä»¶
    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("è¯·å…ˆä»ç½‘é¡µç•Œé¢ç”Ÿæˆå¹¶ä¸‹è½½é…ç½®æ–‡ä»¶")
        return
    except json.JSONDecodeError:
        logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {args.config}")
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {args.config}")
        return
    
    # æå–é…ç½®
    urls = config.get('urls', [])
    depth = config.get('depth', 2)
    format_type = config.get('format', 'html')
    concurrency = config.get('concurrency', 3)
    
    if not urls:
        logger.error("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
        print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
        return
    
    print(f"å°†çˆ¬å– {len(urls)} ä¸ªURLï¼Œæ·±åº¦ä¸º {depth}ï¼Œå­˜å‚¨æ ¼å¼ä¸º {format_type}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    task_info = {
        "start_time": start_time,
        "urls": urls,
        "depth": depth,
        "format": format_type,
        "concurrency": concurrency
    }
    
    # åˆå§‹åŒ–çˆ¬è™«ã€å¤„ç†å™¨å’Œå­˜å‚¨ç®¡ç†å™¨
    crawler = WebCrawler(max_workers=concurrency)
    processor = DataProcessor()
    storage = StorageManager(base_dir='./crawled_data')
    
    # å°†è¿è¡Œç›®å½•è®°å½•åˆ°ä»»åŠ¡ä¿¡æ¯ä¸­
    task_info["run_directory"] = storage.get_run_directory()
    
    try:
        # æ‰¹é‡çˆ¬å–
        print("å¼€å§‹çˆ¬å–...")
        all_results = crawler.batch_crawl(urls, depth)
        print(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_results)} ä¸ªé¡µé¢")
        
        # å¤„ç†ç»“æœ
        processed_content = []
        
        print("å¤„ç†çˆ¬å–å†…å®¹...")
        for url, data in all_results.items():
            # æå–å†…å®¹
            title = data.get("title")
            content = data.get("content")
            html_content = data.get("html")
            status = data.get("status")
            
            # è·³è¿‡æ— æ•ˆå†…å®¹
            if not content and not html_content:
                continue
                
            # å¤„ç†HTMLå†…å®¹
            if html_content:
                # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæˆ–å…¶ä»–ç‰¹æ®Šå†…å®¹
                if isinstance(html_content, str) and (html_content.startswith("PDF_CONTENT_") or 
                                                     html_content.startswith("UNSUPPORTED_CONTENT_")):
                    clean_content = content  # ä½¿ç”¨parse_htmlç”Ÿæˆçš„æè¿°æ€§å†…å®¹
                else:
                    # æ­£å¸¸çš„HTMLå†…å®¹ - æ³¨æ„è¿™é‡Œä¼ å…¥äº†URLä½œä¸ºbase_url
                    clean_html = processor.clean_html(html_content, url)
                    
                    # æå–æ–‡æœ¬æˆ–æ ¼å¼åŒ–HTML
                    if format_type == "txt":
                        clean_content = processor.extract_text_from_html(clean_html)
                    else:
                        clean_content = clean_html
                        
                # æå–å…³é”®è¯ï¼ˆå¯¹äºæ‰€æœ‰å†…å®¹ç±»å‹ï¼‰
                keywords = []
                if content:
                    keywords = processor.extract_keywords(content)
                
                # å­˜å‚¨å†…å®¹
                metadata = {
                    "title": title or "æ— æ ‡é¢˜",
                    "url": url,
                    "depth": data.get("depth", 0),
                    "crawl_time": datetime.now().isoformat(),
                    "keywords": keywords,
                    "content_type": "pdf" if isinstance(html_content, str) and html_content.startswith("PDF_CONTENT_") else "html"
                }
                
                file_path = storage.save_content(
                    url, 
                    clean_content, 
                    format_type, 
                    metadata
                )
                
                # æ„é€ å¤„ç†åçš„å†…å®¹å¯¹è±¡
                processed_item = {
                    "url": url,
                    "title": title or "æ— æ ‡é¢˜",
                    "content": clean_content,
                    "keywords": keywords,
                    "file_path": file_path,
                    "depth": data.get("depth", 0),
                    "format": format_type,
                    "status": status
                }
                
                processed_content.append(processed_item)
        
        # å†…å®¹åˆ†ç±»
        print("å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»...")
        categorized_content = {}
        if processed_content:
            # æå–çº¯æ–‡æœ¬ç”¨äºåˆ†ç±»
            text_contents = [item.get("content", "") for item in processed_content]
            
            # ç¡®å®šåˆ†ç±»æ•°é‡
            cluster_count = min(5, max(2, len(processed_content) // 3))
            
            # æ‰§è¡Œåˆ†ç±»
            clusters = processor.classify_content(text_contents, cluster_count)
            
            # æ•´ç†åˆ†ç±»ç»“æœ
            for cluster_id, indices in clusters.items():
                category_items = [processed_content[idx] for idx in indices]
                categorized_content[str(cluster_id)] = {
                    "id": cluster_id,
                    "items": category_items
                }
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        task_info["end_time"] = end_time
        task_info["duration"] = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        statistics = calculate_statistics(all_results, processed_content, categorized_content, task_info)
        
        # æ„é€ ç»“æœ
        result = {
            "task_info": task_info,
            "content": processed_content,
            "categories": categorized_content,
            "statistics": statistics
        }
        
        # ä¿å­˜ç»“æœ (å°†ç»“æœä¹Ÿä¿å­˜åˆ°è¿è¡Œç›®å½•ä¸‹)
        result_path = os.path.join(storage.get_run_directory(), args.output)
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        # å¦å¤–åœ¨å½“å‰ç›®å½•ä¸‹ä¹Ÿä¿å­˜ä¸€ä»½ç»“æœï¼Œè¿™æ ·ç”¨æˆ·ä»ç„¶å¯ä»¥åœ¨å½“å‰ç›®å½•æ‰¾åˆ°æœ€æ–°çš„ç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        print(f"çˆ¬å–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
        print(f" - {result_path}")
        print(f" - {os.path.abspath(args.output)} (å¤åˆ¶)")
        print(f"æ•°æ®æ–‡ä»¶ä¿å­˜åœ¨: {storage.get_run_directory()}")
        print(f"æ€»å…±çˆ¬å– {len(all_results)} ä¸ªé¡µé¢ï¼ŒæˆåŠŸç‡ {statistics['successRate']}%")
        print(f"ç”¨æ—¶ {task_info['duration']:.2f} ç§’")
        print("è¯·å°†ç»“æœæ–‡ä»¶ä¸Šä¼ åˆ°ç½‘é¡µç•Œé¢æŸ¥çœ‹è¯¦ç»†åˆ†æ")
        
    except KeyboardInterrupt:
        print("\nçˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        print(f"é”™è¯¯: {str(e)}")
    finally:
        # å…³é—­èµ„æº
        crawler.close()

def main_with_urban_legend():
    """é›†æˆéƒ½å¸‚ä¼ è¯´åˆ†æåŠŸèƒ½çš„ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å…¨æ¯æ‹‰æ™®æ‹‰æ–¯äº’è”ç½‘çˆ¬è™«ç³»ç»Ÿ')
    parser.add_argument('-c', '--config', type=str, default='crawler_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', type=str, default='crawler_results.json', help='ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¾“å‡ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('-u', '--urban-legend', action='store_true', help='å¯ç”¨éƒ½å¸‚ä¼ è¯´åˆ†æ')
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # è¯»å–é…ç½®æ–‡ä»¶
    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("è¯·å…ˆä»ç½‘é¡µç•Œé¢ç”Ÿæˆå¹¶ä¸‹è½½é…ç½®æ–‡ä»¶")
        return
    except json.JSONDecodeError:
        logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {args.config}")
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {args.config}")
        return
    
    # æå–é…ç½®
    urls = config.get('urls', [])
    depth = config.get('depth', 2)
    format_type = config.get('format', 'html')
    concurrency = config.get('concurrency', 3)
    
    if not urls:
        logger.error("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
        print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
        return
    
    print(f"å°†çˆ¬å– {len(urls)} ä¸ªURLï¼Œæ·±åº¦ä¸º {depth}ï¼Œå­˜å‚¨æ ¼å¼ä¸º {format_type}")
    
    # å¦‚æœå¯ç”¨éƒ½å¸‚ä¼ è¯´åˆ†æï¼Œåˆå§‹åŒ–åˆ†æå™¨
    urban_legend_analyzer = None
    if args.urban_legend:
        print("å¯ç”¨éƒ½å¸‚ä¼ è¯´åˆ†æåŠŸèƒ½")
        urban_legend_analyzer = UrbanLegendAnalyzer()
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    task_info = {
        "start_time": start_time,
        "urls": urls,
        "depth": depth,
        "format": format_type,
        "concurrency": concurrency,
        "urban_legend_enabled": args.urban_legend
    }
    
    # åˆå§‹åŒ–çˆ¬è™«ã€å¤„ç†å™¨å’Œå­˜å‚¨ç®¡ç†å™¨
    crawler = WebCrawler(max_workers=concurrency)
    processor = DataProcessor()
    storage = StorageManager(base_dir='./crawled_data')
    
    # å°†è¿è¡Œç›®å½•è®°å½•åˆ°ä»»åŠ¡ä¿¡æ¯ä¸­
    task_info["run_directory"] = storage.get_run_directory()
    
    try:
        # æ‰¹é‡çˆ¬å–
        print("å¼€å§‹çˆ¬å–...")
        all_results = crawler.batch_crawl(urls, depth)
        print(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_results)} ä¸ªé¡µé¢")
        
        # å¤„ç†ç»“æœ
        processed_content = []
        
        print("å¤„ç†çˆ¬å–å†…å®¹...")
        for url, data in all_results.items():
            # æå–å†…å®¹
            title = data.get("title")
            content = data.get("content")
            html_content = data.get("html")
            status = data.get("status")
            
            # è·³è¿‡æ— æ•ˆå†…å®¹
            if not content and not html_content:
                continue
                
            # å¤„ç†HTMLå†…å®¹
            if html_content:
                # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæˆ–å…¶ä»–ç‰¹æ®Šå†…å®¹
                if isinstance(html_content, str) and (html_content.startswith("PDF_CONTENT_") or 
                                                     html_content.startswith("UNSUPPORTED_CONTENT_")):
                    clean_content = content  # ä½¿ç”¨parse_htmlç”Ÿæˆçš„æè¿°æ€§å†…å®¹
                else:
                    # æ­£å¸¸çš„HTMLå†…å®¹ - æ³¨æ„è¿™é‡Œä¼ å…¥äº†URLä½œä¸ºbase_url
                    clean_html = processor.clean_html(html_content, url)
                    
                    # æå–æ–‡æœ¬æˆ–æ ¼å¼åŒ–HTML
                    if format_type == "txt":
                        clean_content = processor.extract_text_from_html(clean_html)
                    else:
                        clean_content = clean_html
                        
                # æå–å…³é”®è¯ï¼ˆå¯¹äºæ‰€æœ‰å†…å®¹ç±»å‹ï¼‰
                keywords = []
                if content:
                    keywords = processor.extract_keywords(content)
                
                # æå–åª’ä½“å†…å®¹ä¿¡æ¯
                media = {}
                if html_content and not isinstance(html_content, str) and not (html_content.startswith("PDF_CONTENT_") or html_content.startswith("UNSUPPORTED_CONTENT_")):
                    media = extract_embedded_media(html_content, url)
                
                # éƒ½å¸‚ä¼ è¯´åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
                urban_legend_result = None
                if args.urban_legend and urban_legend_analyzer and clean_content:
                    metadata = {
                        'title': title,
                        'keywords': keywords,
                        'crawl_time': datetime.now().isoformat()
                    }
                    urban_legend_result = urban_legend_analyzer.analyze_content(clean_content, url, metadata)
                    logger.info(f"éƒ½å¸‚ä¼ è¯´åˆ†æç»“æœ {url}: {urban_legend_result['label']}")
                
                # å­˜å‚¨å†…å®¹
                metadata = {
                    "title": title or "æ— æ ‡é¢˜",
                    "url": url,
                    "depth": data.get("depth", 0),
                    "crawl_time": datetime.now().isoformat(),
                    "keywords": keywords,
                    "content_type": "pdf" if isinstance(html_content, str) and html_content.startswith("PDF_CONTENT_") else "html"
                }
                
                # å°†åª’ä½“ä¿¡æ¯æ·»åŠ åˆ°å…ƒæ•°æ®
                if media:
                    metadata["embedded_media"] = media
                    
                # å°†éƒ½å¸‚ä¼ è¯´åˆ†æç»“æœæ·»åŠ åˆ°å…ƒæ•°æ®
                if urban_legend_result:
                    metadata["urban_legend"] = urban_legend_result
                
                file_path = storage.save_content(
                    url, 
                    clean_content, 
                    format_type, 
                    metadata
                )
                
                # æ„é€ å¤„ç†åçš„å†…å®¹å¯¹è±¡
                processed_item = {
                    "url": url,
                    "title": title or "æ— æ ‡é¢˜",
                    "content": clean_content,
                    "keywords": keywords,
                    "file_path": file_path,
                    "depth": data.get("depth", 0),
                    "format": format_type,
                    "status": status,
                    "embedded_media": media if media else None
                }
                
                # æ·»åŠ éƒ½å¸‚ä¼ è¯´åˆ†æç»“æœ
                if urban_legend_result:
                    processed_item["urban_legend"] = urban_legend_result
                
                processed_content.append(processed_item)
        
        # å†…å®¹åˆ†ç±»
        print("å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»...")
        categorized_content = {}
        if processed_content:
            # æå–çº¯æ–‡æœ¬ç”¨äºåˆ†ç±»
            text_contents = [item.get("content", "") for item in processed_content]
            
            # ç¡®å®šåˆ†ç±»æ•°é‡
            cluster_count = min(5, max(2, len(processed_content) // 3))
            
            # æ‰§è¡Œåˆ†ç±»
            clusters = processor.classify_content(text_contents, cluster_count)
            
            # æ•´ç†åˆ†ç±»ç»“æœ
            for cluster_id, indices in clusters.items():
                category_items = [processed_content[idx] for idx in indices]
                categorized_content[str(cluster_id)] = {
                    "id": cluster_id,
                    "items": category_items
                }
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        task_info["end_time"] = end_time
        task_info["duration"] = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        statistics = calculate_statistics(all_results, processed_content, categorized_content, task_info)
        
        # å¦‚æœå¯ç”¨äº†éƒ½å¸‚ä¼ è¯´åˆ†æï¼Œæ·»åŠ ç›¸å…³ç»Ÿè®¡
        if args.urban_legend and urban_legend_analyzer:
            urban_legend_stats = {
                "confirmed_count": 0,
                "suspect_count": 0,
                "normal_count": 0,
                "failed_count": 0
            }
            
            for item in processed_content:
                if "urban_legend" in item:
                    label = item["urban_legend"]["label"]
                    if "å·²ç¡®è®¤éƒ½å¸‚ä¼ è¯´" in label:
                        urban_legend_stats["confirmed_count"] += 1
                    elif "ç–‘ä¼¼éƒ½å¸‚ä¼ è¯´" in label:
                        urban_legend_stats["suspect_count"] += 1
                    elif "æ™®é€šå¸–å­" in label:
                        urban_legend_stats["normal_count"] += 1
                    else:
                        urban_legend_stats["failed_count"] += 1
            
            statistics["urban_legend"] = urban_legend_stats
        
        # æ„é€ ç»“æœ
        result = {
            "task_info": task_info,
            "content": processed_content,
            "categories": categorized_content,
            "statistics": statistics
        }
        
        # ä¿å­˜ç»“æœ (å°†ç»“æœä¹Ÿä¿å­˜åˆ°è¿è¡Œç›®å½•ä¸‹)
        result_path = os.path.join(storage.get_run_directory(), args.output)
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        # å¦å¤–åœ¨å½“å‰ç›®å½•ä¸‹ä¹Ÿä¿å­˜ä¸€ä»½ç»“æœï¼Œè¿™æ ·ç”¨æˆ·ä»ç„¶å¯ä»¥åœ¨å½“å‰ç›®å½•æ‰¾åˆ°æœ€æ–°çš„ç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        print(f"çˆ¬å–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
        print(f" - {result_path}")
        print(f" - {os.path.abspath(args.output)} (å¤åˆ¶)")
        print(f"æ•°æ®æ–‡ä»¶ä¿å­˜åœ¨: {storage.get_run_directory()}")
        print(f"æ€»å…±çˆ¬å– {len(all_results)} ä¸ªé¡µé¢ï¼ŒæˆåŠŸç‡ {statistics['successRate']}%")
        print(f"ç”¨æ—¶ {task_info['duration']:.2f} ç§’")
        
        # å¦‚æœå¯ç”¨äº†éƒ½å¸‚ä¼ è¯´åˆ†æï¼Œæ˜¾ç¤ºåˆ†æç»“æœç»Ÿè®¡
        if args.urban_legend and "urban_legend" in statistics:
            ul_stats = statistics["urban_legend"]
            print("\néƒ½å¸‚ä¼ è¯´åˆ†æç»“æœ:")
            print(f" - å·²ç¡®è®¤éƒ½å¸‚ä¼ è¯´: {ul_stats['confirmed_count']} ç¯‡")
            print(f" - ç–‘ä¼¼éƒ½å¸‚ä¼ è¯´: {ul_stats['suspect_count']} ç¯‡")
            print(f" - æ™®é€šå†…å®¹: {ul_stats['normal_count']} ç¯‡")
            if ul_stats['failed_count'] > 0:
                print(f" - åˆ†æå¤±è´¥: {ul_stats['failed_count']} ç¯‡")
        
        print("è¯·å°†ç»“æœæ–‡ä»¶ä¸Šä¼ åˆ°ç½‘é¡µç•Œé¢æŸ¥çœ‹è¯¦ç»†åˆ†æ")
        
    except KeyboardInterrupt:
        print("\nçˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        print(f"é”™è¯¯: {str(e)}")
    finally:
        # å…³é—­èµ„æº
        crawler.close()

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ è°ƒç”¨
if __name__ == "__main__":
    # å¦‚æœéœ€è¦å¯ç”¨APIåŠŸèƒ½ï¼Œå–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šå¹¶æ³¨é‡Šæ‰main()è°ƒç”¨
    # main_with_api()
    # è¦ä½¿ç”¨å¸¦éƒ½å¸‚ä¼ è¯´åˆ†æçš„ç‰ˆæœ¬ï¼Œå–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šï¼Œå¹¶æ³¨é‡Šæ‰ä¸Šé¢çš„main()è°ƒç”¨
    main_with_urban_legend()

    #main()
